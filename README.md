# ğŸŒ¤ï¸ Pipeline de Dados ClimÃ¡ticos - Airflow

## ğŸ’¼ Contexto de NegÃ³cio

**Desafio**: Uma empresa de turismo em Boston precisava de um sistema automatizado para extrair e processar dados meteorolÃ³gicos semanais, permitindo planejar os melhores roteiros turÃ­sticos baseados nas condiÃ§Ãµes climÃ¡ticas previstas.

**Problema**: A empresa nÃ£o possuÃ­a uma soluÃ§Ã£o escalÃ¡vel para:
- Coletar dados meteorolÃ³gicos de forma consistente e automatizada
- Processar e organizar informaÃ§Ãµes climÃ¡ticas para tomada de decisÃ£o
- Adaptar ofertas de passeios Ã s condiÃ§Ãµes meteorolÃ³gicas previstas

**SoluÃ§Ã£o Desenvolvida**: Pipeline de dados robusto usando Apache Airflow que:
- âœ… Extrai automaticamente dados da API Visual Crossing Weather
- âœ… Processa e estrutura informaÃ§Ãµes meteorolÃ³gicas
- âœ… Armazena dados organizados para anÃ¡lise de negÃ³cio
- âœ… Executa semanalmente de forma automatizada

**Impacto**: Capacita a empresa a tomar decisÃµes data-driven sobre roteiros turÃ­sticos, melhorando a experiÃªncia do cliente e otimizando operaÃ§Ãµes.

---

## ğŸ—ï¸ Arquitetura TÃ©cnica

Este projeto implementa DAGs (Directed Acyclic Graphs) do Apache Airflow para extraÃ§Ã£o e processamento automatizado de dados climÃ¡ticos.

### DAGs Implementados:

### ğŸ¯ 1. `dados_climaticos` - Pipeline Principal
- **Objetivo**: ExtraÃ§Ã£o automatizada de dados meteorolÃ³gicos para anÃ¡lise de negÃ³cio
- **Fonte**: API Visual Crossing Weather (Boston, MA)
- **FrequÃªncia**: Semanal (segundas-feiras Ã s 00:00 UTC)
- **Processamento**:
  - ğŸ“ CriaÃ§Ã£o de estrutura de diretÃ³rios organizados por semana
  - ğŸŒ¡ï¸ ExtraÃ§Ã£o de dados completos da API meteorolÃ³gica
  - ğŸ“Š SeparaÃ§Ã£o em datasets especializados:
    - `dados_brutos.csv`: Dataset completo da API
    - `temperaturas.csv`: MÃ©tricas de temperatura (mÃ¡x, mÃ­n, mÃ©dia)
    - `condicoes.csv`: CondiÃ§Ãµes meteorolÃ³gicas e iconografia

### ğŸ§ª 2. `meu_primeiro_dag` - Exemplo Educacional
- **Objetivo**: DemonstraÃ§Ã£o de conceitos fundamentais do Airflow
- **Funcionalidades**: Tarefas sequenciais e paralelas, criaÃ§Ã£o de estruturas de dados

## ï¿½ï¸ Stack TecnolÃ³gica

- **OrquestraÃ§Ã£o**: Apache Airflow 2.0+
- **Linguagem**: Python 3.7+
- **Processamento**: pandas para manipulaÃ§Ã£o de dados
- **Scheduling**: pendulum para gerenciamento de tempo
- **API Integration**: Visual Crossing Weather API
- **Versionamento**: Git + GitHub

## ğŸ† CaracterÃ­sticas TÃ©cnicas

- **Arquitetura orientada a eventos** com DAGs modulares
- **Processamento automatizado** com agendamento robusto
- **Tratamento de dados estruturados** em mÃºltiplos formatos
- **IntegraÃ§Ã£o com APIs externas** com tratamento de erros
- **Logging e monitoramento** nativo do Airflow
- **Estrutura escalÃ¡vel** para expansÃ£o futura

## ï¿½ Casos de Uso e ExtensÃµes

### AplicaÃ§Ãµes Atuais
- ğŸ¯ **Planejamento TurÃ­stico**: OtimizaÃ§Ã£o de roteiros baseada em condiÃ§Ãµes meteorolÃ³gicas
- ğŸ“Š **AnÃ¡lise Temporal**: IdentificaÃ§Ã£o de padrÃµes climÃ¡ticos semanais
- ğŸ”” **Alertas AutomÃ¡ticos**: Base para sistemas de notificaÃ§Ã£o meteorolÃ³gica

### PossÃ­veis ExpansÃµes
- ğŸŒ **Multi-cidade**: ExtensÃ£o para mÃºltiplas localidades
- ğŸ¤– **ML Integration**: Modelos preditivos baseados em histÃ³rico
- ğŸ“± **API prÃ³pria**: Desenvolvimento de endpoints para consumo
- ğŸ“§ **NotificaÃ§Ãµes**: Sistema de alertas automÃ¡ticos
- ğŸ”— **IntegraÃ§Ã£o CRM**: ConexÃ£o com sistemas de gestÃ£o de clientes

---

## ğŸš€ Quick Start

1. Clone o repositÃ³rio:
```bash
git clone <url-do-repositorio>
cd airflowalura
```

2. Instale as dependÃªncias:
```bash
pip install apache-airflow pandas pendulum
```

3. Configure o Airflow:
```bash
# Inicializar o banco de dados do Airflow
airflow db init

# Criar usuÃ¡rio admin (se necessÃ¡rio)
airflow users create \
    --username admin \
    --firstname Admin \
    --lastname User \
    --role Admin \
    --email admin@example.com
```

## ğŸ”§ ConfiguraÃ§Ã£o

### API Key da Visual Crossing Weather

Para usar o DAG `dados_climaticos`, vocÃª precisa:

1. Obter uma API key gratuita em [Visual Crossing Weather](https://www.visualcrossing.com/weather-api)
2. Substituir a chave no arquivo `dags/dados_crimaticos.py`:
```python
key = "SUA_API_KEY_AQUI"
```

### Estrutura de DiretÃ³rios

O projeto gera a seguinte estrutura de dados:
```
airflowalura/
â”œâ”€â”€ semana=YYYY-MM-DD/
â”‚   â”œâ”€â”€ dados_brutos.csv
â”‚   â”œâ”€â”€ temperaturas.csv
â”‚   â””â”€â”€ condicoes.csv
â””â”€â”€ pasta=YYYY-MM-DDTHH:MM:SS+00:00/
```

## ğŸƒâ€â™‚ï¸ Como Executar

1. Inicie o servidor web do Airflow:
```bash
airflow webserver --port 8080
```

2. Em outro terminal, inicie o scheduler:
```bash
airflow scheduler
```

3. Acesse a interface web em `http://localhost:8080`

4. Ative os DAGs desejados na interface web

## ğŸ“Š Dados e MÃ©tricas

### Fonte de Dados
- **API**: Visual Crossing Weather
- **LocalizaÃ§Ã£o**: Boston, Massachusetts, USA
- **Cobertura**: PrevisÃ£o de 7 dias
- **AtualizaÃ§Ã£o**: Semanal automatizada
- **Volume**: ~1000 registros/mÃªs

### Datasets Gerados
| Arquivo | DescriÃ§Ã£o | Campos Principais |
|---------|-----------|-------------------|
| `dados_brutos.csv` | Dataset completo da API | Todos os campos meteorolÃ³gicos |
| `temperaturas.csv` | MÃ©tricas tÃ©rmicas | datetime, tempmax, tempmin, temp |
| `condicoes.csv` | CondiÃ§Ãµes meteorolÃ³gicas | datetime, description, icon |

### Estrutura de SaÃ­da
```
airflowalura/
â”œâ”€â”€ semana=2025-09-15/
â”‚   â”œâ”€â”€ dados_brutos.csv      # Dataset completo
â”‚   â”œâ”€â”€ temperaturas.csv      # AnÃ¡lise tÃ©rmica
â”‚   â””â”€â”€ condicoes.csv         # CondiÃ§Ãµes climÃ¡ticas
â””â”€â”€ semana=2025-09-22/
    â”œâ”€â”€ dados_brutos.csv
    â”œâ”€â”€ temperaturas.csv
    â””â”€â”€ condicoes.csv
```

## ğŸ“ Estrutura do Projeto

```
airflowalura/
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ dados_crimaticos.py    # DAG principal para dados climÃ¡ticos
â”‚   â””â”€â”€ meu_primeiro_dag.py    # DAG de exemplo
â”œâ”€â”€ logs/                      # Logs do Airflow
â”œâ”€â”€ airflow.cfg               # ConfiguraÃ§Ã£o do Airflow
â”œâ”€â”€ airflow.db                # Banco de dados SQLite do Airflow
â”œâ”€â”€ webserver_config.py       # ConfiguraÃ§Ã£o do servidor web
â””â”€â”€ README.md                 # Este arquivo
```

## ğŸ”§ PersonalizaÃ§Ã£o

### Alterando a Cidade
Para extrair dados de outra cidade, modifique a variÃ¡vel `city` em `dags/dados_crimaticos.py`:
```python
city = 'SÃ£o Paulo'  # ou qualquer outra cidade
```

### Alterando a FrequÃªncia
Para mudar o agendamento, altere o `schedule_interval`:
```python
schedule_interval="0 0 * * *"  # Para execuÃ§Ã£o diÃ¡ria
schedule_interval="@weekly"    # Para execuÃ§Ã£o semanal
```

## ğŸ¤ ContribuiÃ§Ã£o

1. FaÃ§a um fork do projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

---

## ğŸ“ LicenÃ§a & Contato

**LicenÃ§a**: MIT License - Projeto educacional e demonstrativo

**Desenvolvedor**: [@tmarsbr](https://github.com/tmarsbr)

**Contato**: Para dÃºvidas ou discussÃµes tÃ©cnicas, abra uma [issue](https://github.com/tmarsbr/airflowalura/issues) no repositÃ³rio.

---

*Este projeto demonstra competÃªncias em engenharia de dados, orquestraÃ§Ã£o de workflows, integraÃ§Ã£o de APIs e automaÃ§Ã£o de processos usando ferramentas modernas da stack de dados.*